YouTube Transcript for Video: LLMs and AI Agents Transforming Unstructured Data
Video ID: _pEEJu-2KKM
Generated on: 2025-05-21 23:30:31
==================================================

[00:00] Written language is one of humanity's
most important
[00:04] and transformative technologies.
[00:06] We may not think about writing
or alphabets as technology,
[00:11] but beginning with cave paintings
[00:14] and working forward into hieroglyphics,
[00:18] cuneiform, Gutenberg's movable
[00:21] type printing press, Xerox copies.
[00:25] And now, even with modern digital images
like the Portable Document File,
[00:29] throughout history, people have written
important things down.
[00:35] We tend to record important things
that happen in important
[00:39] and complicated collections of facts
in written form.
[00:44] Today, we live in a data driven world,
and as developers and technologists,
[00:48] we're trying to develop ways
[00:50] to support decision
making that can be very data driven.
[00:55] The challenge that we have is
that documents are unstructured.
[00:59] And so from an engineering
[01:00] or developer perspective,
documents are unstructured data.
[01:04] What we're trying to do to leverage
document
[01:07] data in decision
making is to structure the data.
[01:11] And to do this
we need to talk about how we can process
[01:15] raw, unstructured data to create
highly structured, usable data.
[01:19] Or rather,
how can we use very powerful tools
[01:23] that can they can enhance
and improve our ability
[01:26] to have humans working
directly with unstructured data.
[01:31] Today we're going to take a look
at two of my favorite things.
[01:34] AI agents and document intelligence.
[01:38] Before we jump into the AI conversation,
let's make sure that we frame
[01:43] the document
problem as a data challenge.
[01:47] So we all know
you know what a document is, right?
[01:50] So here we may have a document.
[01:52] We'll call it document one.
[01:54] And you know inside this document
you know
[01:57] documents tend to have like a title
and may have a lot of words.
[02:00] Some of our words are short
and some of our words are long.
[02:03] If we're lucky, maybe this thing's got
some structure, like some paragraphs.
[02:07] It's going to have some punctuation,
throughout, etc..
[02:12] Now, we know in, 
[02:15] in a variety of environments,
business environments that we're going
[02:19] to also come in to documents, that have
a lot of words like document one.
[02:24] But then, you know, they're going
to have tabular data oftentimes.
[02:28] So we may have,
examples of tabular data,
[02:33] you know, like a,
like a two by four table.
[02:35] And then we may have a more complicated
table down here
[02:39] that looks like a
what we did a five by three down here.
[02:42] Okay.
[02:43] Now we know that
[02:45] documents are going to contain,
you know, words and punctuation.
[02:49] We know they're going to contain
tabular data.
[02:51] But another thing
that we're going to see is,
[02:54] you know, some documents are are short
and some documents
[02:58] can really get very long,
can get long and quite complicated.
[03:02] Right.
[03:02] So we can get documents
that are 600 pages plus word counts
[03:06] really start to take off on us.
[03:08] And then over here,
you know, document four,
[03:11] may be an example of something
like document two
[03:14] where we've got words,
you know, in tables everywhere.
[03:18] And we can start getting
into complexities like tables
[03:23] that run across pages.
[03:25] Right.
[03:26] So we can get into lots and lots of
pages, you know, with long tables.
[03:30] You know, maybe we have tables,
you know, with in excess
[03:33] of even 10,000 rows.
[03:35] So this is this is our document space.
[03:37] And so this this can be viewed
[03:40] as traditionally
as an unstructured data problem.
[03:43] Okay.
[03:44] And, and when we run into practitioners
who have,
[03:46] who have wrestled with this problem
in the past, the technology
[03:50] we always hear about
is optical character recognition.
[03:54] And what optical
character recognition does
[03:57] is it's able to use computer vision
to go through this document
[04:01] and to recognize,
[04:04] different characters and words,
[04:07] and it's able to translate these
into text.
[04:11] With OCR, you know, you can even have
some look, perhaps, recognizing tables.
[04:16] But we get into real trouble.
[04:19] You know, when we run into things
like page breaks
[04:22] and when we and we
when we translate data within OCR,
[04:26] we don't have any real semantic
understanding of what we've done.
[04:30] We've really just accomplished
creating a lot of text.
[04:32] Okay.
[04:33] So let's keep that problem in mind.
[04:36] The other thing that we want to talk
about with documents
[04:38] is that any
one document is not that valuable,
[04:42] because documents
tend to relate to one another.
[04:45] Okay.
[04:46] So let's talk for a quick minute
about what we call hierarchies.
[04:53] Okay.
[04:54] And with
hierarchies we have vertical hierarchies
[04:58] and horizontal hierarchies okay.
[05:02] And what we mean by hierarchies
or how documents
[05:06] in a population relate to one another
to create a logical whole.
[05:10] Here are a few examples.
[05:11] Let's imagine that we're in
sort of a financial legal use case,
[05:16] and we have a master service agreement
as a contract.
[05:20] But then underneath this master service
agreement, we have a statement of work.
[05:25] And then this statement of work
is later amended.
[05:29] And then even later still,
we have a statement of work ..2
[05:35] And this statement of work
can also be amended and so on.
[05:39] So now as we as we look at this,
we say, okay, well what can happen next?
[05:43] A statement of work
may spawn a purchase order,
[05:47] and a purchase order
may eventually have an invoice.
[05:51] So when we think about
vertical hierarchy, we're talking about
[05:54] having to understand
this set of documents all together
[05:57] to understand the whole of the
meaning of this relationship.
[06:02] And then we start to
[06:03] have horizontal hierarchies
that require us
[06:06] to look at different document types
and how they relate to one another.
[06:10] We can use two more, 
really quick examples here.
[06:13] So if I'm in an R&D, engineering space,
I may have a research paper,
[06:18] and then later on we may have
another important research result.
[06:22] We'll call that R2.
[06:24] And that result may site work
or results in the first result.
[06:28] And then later on
we may have another research R3
[06:33] and it may cite R2.
[06:36] And the original paper
[06:38] As an example of a horizontal hierarchy,
[06:40] We may eventually work outward
and get to, a patent filing.
[06:45] So this might be a US patent.
[06:47] And later still we may have productization and product documentation.
[06:51] So again in this example,
we have a lot of research work
[06:56] that gives us
sort of the epistemology of the ideas
[07:00] and, and the in the citations
of who is creating the ideas.
[07:03] And then in the horizontal space
we have the evolution towards
[07:07] US patent filings and productization.
[07:10] A last very short example
would be people who move physical goods
[07:14] around the world, supply chain type
people where we have bills of lading,
[07:20] and when we ship something expensive,
we probably have
[07:24] a certificate of insurance that shows
that we've insured what we're shipping,
[07:28] and then we're going to eventually
send this to someone who's going to have
[07:31] a receiving, a shipping receipt
of what they believe they received.
[07:35] And if they think they received
something that was damaged,
[07:39] they're going to file a claim.
[07:41] And the claim is going to relate
to who we shipped to,
[07:44] and it's going to relate
to our certificate of insurance.
[07:46] So again, to understand the relationship
[07:49] of the shipper, we need to understand
the emails and the CEOs.
[07:53] And then to understand the
the horizontal relationships,
[07:56] we need to understand how e bills relate
to shipping receipts
[07:59] and how the certificate of insurance
relates to a claim.
[08:04] So we've introduced documents
as a data problem,
[08:08] and we've discussed how documents
are fundamentally constructed
[08:13] with language and data types
like numbers and dates.
[08:17] Okay,
so let's talk about the breakthrough.
[08:20] The breakthrough
and the big new tool that we have today
[08:24] are these GPT models,
which are found, foundation
[08:28] models that allow us to develop these
these large language models.
[08:32] And GPT stands for generative
[08:35] pre-trained transformer.
[08:38] Okay.
[08:38] So a lot of the technology
that's in these GPT
[08:42] models is borrowed from, neural nets.
[08:46] But what we what we have is
we have the ability to apply,
[08:50] these transformers to a language
which is finite.
[08:55] So let's talk about the English language
here.
[08:58] So in the English language
we have about 170,000.
[09:04] It's a little more than that.
[09:05] But we have about 170,000 words
that are in the active vocabulary.
[09:10] Right.
[09:11] We also know that we have,
you know, a, b c dot
[09:15] dot dot to see, you know,
so we know we've got 26, characters.
[09:21] And then of course we,
we know that we have numbers.
[09:24] You know, numbers can be represented
different ways, right.
[09:27] Like you can have a decimal 5.1
or whatever.
[09:30] But ultimately, the numbers,
the numbers we know, to be infinite,
[09:34] but they're somewhat easy to recognize.
[09:38] So, so basically you've got this,
you've got this mostly finite space,
[09:42] finite when we come to language,
still infinite when we come to numbers.
[09:46] But ultimately we've got we've
got an area that we can work in here.
[09:51] And when we
when we consider these numbers
[09:54] and we look at some of the open
source Lims that are on the market,
[09:58] we know that this type of a space
[10:02] ends up being, greater than,
[10:07] greater than 600 billion
[10:11] parameters.
[10:15] So to parameterize
[10:17] a space that looks like
the English language, we end up with,
[10:22] a little over
between 600 and 700 billion
[10:26] parameters. Okay.
[10:27] Now, let's let's jump for a minute into.
[10:30] Okay.
[10:31] We talked about,
generative pre-trained transformers.
[10:34] Let's very briefly hit on what what
that's all about.
[10:38] So basically,
you know, we're trying to have inputs.
[10:43] We want a machine that can take inputs
[10:48] and give us, you know, expected outputs.
[10:51] Right.
[10:51] So we're trying to create inputs
and outputs.
[10:53] That makes sense.
[10:55] Our inputs are going to be language
right.
[10:57] They're going to be we're
going to call them tokens later on.
[11:00] But basically
we're trying to do embedding
[11:03] which is which is taking vocabulary
and turning it into math.
[11:07] So this is this is basically one
dimensional vectors,
[11:11] sort of looking at the
[11:12] concept of distance between things
in a one dimensional space.
[11:16] And then from here
we start using transformers.
[11:19] And what transformers do is
they start to create,
[11:22] a really high dimensional space
so that instead of just looking,
[11:28] at the distance,
you know, between two things,
[11:31] you start to see
two dimensional representations,
[11:35] you know, that look,
something more like this
[11:37] if you've ever, you know, looked at
any of the literature that's out there.
[11:40] So this is kind of like 1D
and this is like a 2D matrix,
[11:43] you know, representation
of a multi dimensional space okay.
[11:47] Attention and normalization
start to deal with grouping.
[11:52] So this idea of grouping
and normalization
[11:57] is this
is this fancy way to move from like one
[11:59] dimensional vectors
up into really big matrices
[12:03] and then chunking things
into like smaller groups of matrices.
[12:07] Okay.
[12:08] This is all fancy math that you can go,
look into more if you'd like to.
[12:12] And then when we get to softmax
and attention output,
[12:17] softmax is a really fancy
probabilistic algorithm
[12:21] that basically looks at input tokens
[12:24] and then does a bunch
of this computation to create an output,
[12:28] a set of output tokens
that will have probability one, meaning
[12:32] this is the thing that's sort of
determining what is the most
[12:37] likely thing
[12:38] that you're expecting to see based
on what you put in the attention output.
[12:43] As we said, is kind of related to
the most likely, an attention output.
[12:47] This is the layer
[12:48] where you say, hey, I want my answers
to sound like you're from New York.
[12:52] Or I want my answers to sound
like you're from Silicon Valley.
[12:55] So if you're going to do stylistic
type of outputs, this is the layer that
[12:58] you're usually operating in.
[13:00] And then your,
[13:01] when you project vocabulary,
this is, this is literally your output.
[13:06] Right.
[13:06] So we have this
we have this new exciting tech.
[13:11] And we go back to our problem
of having a document.
[13:14] Okay. So we have a document.
[13:16] And actually we we have a set
a lot of documents.
[13:19] But let's look at one document here.
[13:21] And this document
may have a thousand words in it.
[13:24] Okay.
[13:25] What we're trying to get to is
we're trying to get to
[13:29] a data model
representation of this document
[13:34] and the data model that we care about
might be small.
[13:38] It might we might really care
about 20 really key data points
[13:42] or 50 key data points out of this,
out of this larger document.
[13:47] The mistake that people make
is that to get from here to here,
[13:51] we we tend to want to think
about a reductionist process.
[13:55] You know, we're taking
we're taking this document
[13:58] and we're whittling it down, you know,
[14:01] to find these couple of key points
that we want to end up with over here.
[14:05] But in reality, what happens is
we have this huge expansion
[14:09] that's happening, right?
[14:10] So first, you know, we may apply an OCR.
[14:13] And as we apply an OCR right,
the data expands
[14:19] from a thousand data points to maybe
1 million or 10 million data points.
[14:25] Then we apply
some natural language processing
[14:29] and we develop still more data.
[14:31] So we're expanding.
[14:33] And then in the end
we get to something like an LLM
[14:37] And then we,
we deal with even more data still.
[14:40] So really what we're doing is we've got
this we've got this expansion,
[14:45] we've got this expansion
that's happening.
[14:47] And then and then that's
going to allow us to get back
[14:51] to contracting to this data model,
[14:54] which is where which is where
we're going to make everybody happy.
[14:58] Right?
Is if we can really get this, this,
[14:59] this representation of the data
that we care about.
[15:02] Okay.
[15:03] So this is our data layer.
[15:05] This is our LLM breakthrough.
[15:07] And these are some of the older
technologies that work together
[15:10] in this overall, pipeline.
[15:13] And so what remains for us is to talk
about how we can develop
[15:17] a genetic workflows or develop agents,
[15:20] to, to do this work for us.
[15:23] Now that we've talked about some of the
newer technologies, in particular LLM’s
[15:27] and how LLM’s can play with
[15:29] other technologies, we're more familiar
with, like OCR and NLP.
[15:33] Now let's talk about how this comes
together.
[15:36] In developing agents and what agenetic
[15:39] workflows are versus
a traditional workflows.
[15:42] So let's let's start by naming
a couple of agents and thinking about
[15:45] what would be some useful agents.
[15:47] So we can have an inspection agent.
[15:50] And an inspection agent could take
a file and do some deep file inspection.
[15:56] Right.
[15:56] So an inspection
agent may take checksums.
[15:59] It may look at word
spacing within files.
[16:02] It may look at file length.
[16:04] It may look at file size.
[16:05] It may look at aspects of what kind
of contents are in the file okay.
[16:11] Another agent that could be useful
could be an OCR agent.
[16:14] So we go out
and we take our most performant
[16:16] OCR engine that we really like.
[16:18] Or in some cases,
if you have a multi multi-modal model,
[16:22] some,
some folks are starting to look at Lims
[16:25] themselves
as being able to replace the OCR.
[16:28] But essentially
this agent can take our image data and,
[16:33] and transform it into, you know,
[16:36] text data,
alphanumeric data tables, etc..
[16:40] We could have a
[16:42] vectorized agent, 
vectorized agent is is probably almost
[16:47] certainly pretty heavily involved, 
with, with an LLM of our choice.
[16:52] The vectorized agent is going to
is going to chunk
[16:55] and chunk up our document into tokens,
[16:58] in groupings
and run it through an LLM to develop
[17:02] some of that vectorized magic
that we just spoke about.
[17:06] The splitter agents,
we can make a splitter agent here.
[17:09] Here it is.
[17:10] The splitter agents could be, an agent
[17:14] that takes a look at everything
we've done up to this point
[17:17] and makes determinations and learns
where documents should be split.
[17:22] Right.
[17:22] So we may find that we want to split
[17:25] documents into into multiple,
multiple documents.
[17:29] In cases where maybe we got sloppy and,
and combined
[17:33] a lot of multiple documents
into one before we processed.
[17:36] The extraction or an extract
[17:39] agent can be another agent
that we create.
[17:43] And the extract agent, you know,
could be key in taking all that data
[17:48] that we described and helping us
get back down to identifying
[17:52] that really critical data model,
that we talked about.
[17:56] So the extract agent is going to
is going to have a lot of prompting.
[18:00] It's going to have
a lot of automated prompting.
[18:02] That's going to line up
with the data model,
[18:05] to allow us to identify
those key data points.
[18:08] And then, you know, lastly,
with our little example
[18:10] we're working through here, maybe
we have a match or a matching agent,
[18:14] and the matching agent, you know,
has access to all this metadata.
[18:19] And the matching agent
is doing some of that magic
[18:23] that we were talking about,
[18:24] you know, to help us establish those
horizontal and vertical hierarchies
[18:28] so that we take our swarm of documents
and we start to understand
[18:32] the documents, you know, logically, 
holistically, on a logical standpoint
[18:37] or more, more, horizontally
in terms of transactional standpoint.
[18:41] So if we if we kind of
look at how this is arranging itself
[18:45] and we're, it's
tempting to think of agentic workflows
[18:50] in sort of a linear framework,
[18:52] which is similar to how we would
probably set up a data pipeline prior
[18:56] to taking an AI approach, right,
where we tend to think about
[19:00] inputs and outputs.
[19:01] But an output from one stage becomes an
input to the next stage and an output,
[19:05] and then the output becomes
an input to the next stage, and so on.
[19:09] And so what
we're really getting to today
[19:12] when we think about rearranging,
these types of sequential workflows,
[19:17] which we might describe
as deterministic,
[19:20] okay, what we're really getting to today
is, you know, maybe we can arrange
[19:23] a generic workflows
which are more autonomous
[19:27] and which are triggered by events
such as new data arriving into,
[19:31] the area where we're giving
these agents scope to operate.
[19:36] And, and perhaps we can create
interactions where these agents
[19:40] are looking
at the work of the other agents,
[19:42] and then they're doing
their useful piece of the work.
[19:45] Right.
[19:46] And so what we what the possibility
that we open up here
[19:49] is not only autonomy,
which could lead to efficiency
[19:53] and computing resource use, scalability.
[19:56] But we're also kind of entering
this non-deterministic,
[20:00] non-deterministic space, 
which, which potentially
[20:04] opens up a lot of new possibilities
that we haven't yet considered.
