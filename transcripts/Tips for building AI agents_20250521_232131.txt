YouTube Transcript for Video: Tips for building AI agents
Video ID: LP5OCa20Zpg
Generated on: 2025-05-21 23:21:33
==================================================

[00:00] - I feel like agents for consumers
[00:01] are like fairly over hyped right now.
[00:02] - Okay, here we go. Hot take.
[00:04] - Trying to have a agent
fully book a vacation for you,
[00:08] almost just as hard as just
going and booking it yourself.
[00:10] - Take one. Mark.
[00:12] - Today we're going behind the scenes
[00:13] on one of our recent blog posts,
Building Effective Agents.
[00:17] I'm Alex, I lead Claude
Relations here at Anthropic.
[00:20] I'm Erik, I'm on the
research team at Anthropic.
[00:22] - I'm Barry, I'm on the Applied AI team.
[00:24] - I'm gonna kick us off here.
[00:25] For viewers just jumping in,
[00:28] what's the quick version of
what an agent actually is?
[00:31] I mean, there's a
million definitions of it
[00:33] and why should a developer
[00:35] or somebody that's
actually building with AI
[00:37] care about these things?
[00:39] Erik, maybe we can start with you.
[00:41] - Sure.
[00:42] Yeah, so I think something
we explored in the blog post
[00:44] is that first of all, a lot
of people have been saying
[00:46] everything is an agent,
referring to almost anything
[00:49] more than just a single LLM call.
[00:51] One of the things we tried
to do in the blog post
[00:53] is really kind of
separate this out of like,
[00:55] hey, there's workflows,
[00:57] which is where you have a few
LLM calls chained together.
[01:00] And really what we think an agent is
[01:03] is where you're letting the LLM decide
[01:05] sort of how many times to run.
[01:07] You're having it continuing to loop
[01:08] until it's found a resolution.
[01:10] And that could be, you
know, talking to a customer
[01:13] for customer support,
[01:14] that could be iterating on code changes.
[01:16] But something where like you don't know
[01:18] how many steps it's
gonna take to complete,
[01:20] that's really sort of
what we consider an agent.
[01:22] - Interesting, so in the
definition of an agent,
[01:25] we are letting the LLM
kind of pick its own fate
[01:28] and decide what it wants
to do, what actions to take
[01:30] instead of us predefining
a path for it. Exactly.
[01:34] - It's more autonomous, whereas a workflow
[01:35] you can kind of think
of it as like, you know,
[01:39] yeah, a workflow or sort
of like it's on rails
[01:42] through a fixed number of steps.
[01:43] - I see.
[01:44] So this distinction, I
assume this was the result
[01:48] of many, many conversations with customers
[01:50] and working with different teams
[01:51] and even trying things ourself.
[01:53] Barry, can you speak more to
maybe what that looks like
[01:56] as we got to create this divide
[01:59] between a workflow and an agent?
[02:00] And what sort of patterns
surprised you the most
[02:02] as you were going through this?
[02:04] - Sure.
[02:05] Honestly, I think all
of this kind of evolved
[02:07] as like model got better
[02:08] and like teams got more sophisticated.
[02:10] We both worked with a large number of
[02:12] customers who are very sophisticated
[02:13] and we kind of went
from having a single LLM
[02:16] to having a lot of LLMs
[02:17] and like eventually having LLMs
[02:18] orchestrating themselves.
[02:20] So, you know, one of the reasons
[02:22] why we decide to create this distinction
[02:24] is because we started to see
these two distinct patterns
[02:26] where you have workflows
that's pre orchestrated by code
[02:29] and then you also have, you know, agent,
[02:30] which is like a simpler but
complex in in other sense,
[02:35] like different shape that
we're starting to see.
[02:38] Really, I think like as the models
[02:40] and all of the tools start to get better,
[02:42] you know, agents are becoming
more and more prevalent
[02:44] and more and more capable.
[02:46] And that's why we decided hey,
[02:47] this is probably a good time for us
[02:49] to give a formal definition.
[02:51] - So in practice, if you're a developer
[02:53] implementing one of these things,
[02:54] what would that actually
look like in your code
[02:57] as you're starting to build this?
[02:58] Like, the differences between,
[03:00] like maybe we actually go down
[03:02] to like the prompt level here.
[03:03] What does an agent
prompt look like or flow
[03:07] and what does a workflow look like?
[03:08] - Yeah, so I think a
workflow prompt looks like,
[03:12] you have one prompt, you
take the output of it,
[03:15] you feed it into prompt B,
take the output of that,
[03:17] feed it into prompt C,
and then you're done.
[03:19] Kind of, there's this straight
line fixed number of steps.
[03:22] You know exactly what's gonna happen
[03:25] and maybe you have some
extra code that sort of
[03:27] checks the intermediate results of these
[03:29] and make sure they're okay.
[03:31] But you kind of know
exactly what's gonna happen
[03:33] in one of these paths.
[03:35] And each of those prompts
[03:38] is sort of a very specific prompt,
[03:39] just sort of taking one input
[03:41] and transforming it into another output.
[03:44] For instance, maybe
one of these prompts is
[03:46] taking in the user question
[03:48] and categorizing it into
one of five categories
[03:51] so that then the next prompt
can be more specific for that.
[03:55] In contrast, an agent
prompt will be sort of
[03:58] much more open-ended and
usually give the model tools
[04:02] or multiple things to check and say,
[04:04] Hey, here's the question,
[04:05] and you can do web searches or
you can edit these code files
[04:09] or run code and keep doing
this until you have the answer.
[04:13] - I see. So there's a few
different use cases there.
[04:18] That makes sense as we start to arrive
[04:19] at like these different conclusions.
[04:21] I'm curious as we've now kind
of covered at a high level
[04:24] how we're thinking about these,
these workflows and agents
[04:27] and talking about the blog post.
[04:30] I want to dive even
further behind the scenes.
[04:33] Were there any funny stories
Barry, of wild things
[04:35] that you saw from customers
that were interesting
[04:38] or just kind of far out there in terms of
[04:41] how people are starting to actually use
[04:42] these things in production?
[04:44] - Yeah, this is actually
from my own experience
[04:46] like building agents.
[04:48] I joined like about a month
before the Sonnet v2 refresh
[04:52] and one of my onboarding
tasks was to run OSWorld,
[04:55] which was a computer use benchmark.
[04:57] And for a whole week me
and this other engineer
[05:00] were just staring at agent trajectories
[05:03] that were counterintuitive to us
[05:05] and then well, you know, we weren't sure
[05:07] why the model was making
the decision it was,
[05:10] given the instructions
that we would give it.
[05:12] So we decided we're gonna act like Claude
[05:15] and, you know, put ourselves
in that environment.
[05:17] So we would do this really silly thing
[05:18] where we close our eyes for a whole minute
[05:21] and then we're like blink
at the screen for a second
[05:23] and we close our eyes
again and just think,
[05:25] well, I have to write Python code
[05:26] to operate in this
environment, what would I do?
[05:28] And suddenly it made a lot more sense.
[05:30] And I feel like a lot of agent design
[05:32] comes down to that.
[05:33] It's like, there's a lot of
context and a lot of knowledge
[05:35] that the model maybe does not have
[05:38] and we have to be empathetic to the model
[05:41] and we have to make a lot of that clear
[05:43] in the prompt, in the tool description,
[05:44] and in the environment.
[05:45] - I see, so a tip here for
developers is almost to act
[05:49] as if you are looking through
the lens of the model itself
[05:52] in terms of like, okay,
[05:53] what would be the most
applicable instructions here?
[05:55] How is the model seeing the world,
[05:57] which is very different than
how we operate as a human,
[06:00] I guess, with additional context.
[06:04] Erik, I'm curious if you
have any other stories
[06:06] that you've seen.
[06:07] - Yeah, I think actually
my, in a very similar vein,
[06:10] I think a lot of people
really forget to do this.
[06:14] And I think maybe the
funniest things I see
[06:16] is that people will put a lot of effort
[06:19] into creating these really
beautiful, detailed prompts
[06:23] and then the tools that
they make to give the model
[06:25] are sort of these incredibly bare bones,
[06:28] you know, no documentation,
[06:31] the parameters are named A and B.
[06:33] And it's kind of like, oh, an engineer
[06:35] wouldn't be able to, you
know, work with this.
[06:37] - Right.
[06:39] - You know, work with this
as if this was a function
[06:41] they had to use.
[06:42] 'Cause there's no documentation,
[06:44] how can you expect Claude
to use this as well?
[06:46] So kind of it's that lack
[06:48] of like putting yourself
in the model's shoes.
[06:49] And I think a lot of people
[06:51] when they start trying to use
tool use and function calling,
[06:56] they kind of forget that
they have to prompt as well
[06:58] and they think about
the model just as this,
[07:02] you know, a more classical
programming system.
[07:04] But it is still a model
[07:05] and you need to be prompt engineering
[07:07] in the descriptions of
your tools themselves.
[07:10] - Yeah, I've noticed that.
[07:11] It's like people forget
[07:12] that it's all part of the same prompt.
[07:14] Like, it's all getting
fed into the same prompt
[07:16] in the context window and
writing a good tool description
[07:19] influences other parts
of the prompt as well.
[07:20] So that is one aspect to consider.
[07:25] Agents is this kind of all
the hype term right now,
[07:29] a lot of people are talking about it
[07:30] and there's been plenty
of articles written
[07:33] and videos made on the subject.
[07:36] What made you guys think
that now is the right time
[07:39] to write something ourselves
[07:40] and talk a little bit more
about the details of agents?
[07:45] - Sure, yeah.
[07:46] I think one of the, you know,
most important things for us
[07:49] just to be able to explain things well.
[07:51] I think that's a big
part of our motivation,
[07:54] which is we walk into customer meetings
[07:56] and everything is referred
to as a different term
[07:58] even though they share the same shape.
[08:00] So we thought, you know,
it would be really useful
[08:02] if we can just have a set of definition
[08:04] and a set of diagrams
[08:06] and code to explain these
things to our customers.
[08:09] And you know, we are getting to the point
[08:11] where the model is capable
[08:13] of doing a lot of the agentic workflows
[08:16] that we're seeing.
[08:17] And that seems like, you
know, the right time for us
[08:20] to, you know, have some definitions
[08:22] or just to make these
conversations easier.
[08:25] - Yeah, I think for me, I saw
[08:27] that there was a lot of
excitement around agents,
[08:28] but also a lot of people
really didn't know
[08:31] what it meant in practice
[08:32] and so they were trying to bring agents
[08:34] to sort of any problem they had,
[08:36] even when much simpler systems would work.
[08:39] And so I saw that as one of the reasons
[08:40] that we should write this as,
[08:42] guide people about how to do agents
[08:44] but also where agents are appropriate.
[08:46] And that you shouldn't go
after a fly with a bazooka.
[08:48] - I see, I see.
[08:50] So that was a perfect parlay
into my next question here.
[08:54] There's a lot of talk about
the potential of agents
[08:56] and every developer out
there and every startup
[08:59] and business is trying to think about
[09:00] how they can build their
own version of an agent
[09:02] for their company or product.
[09:06] But you guys are starting to see
[09:07] what actually works in production.
[09:08] So we're gonna play a little game here.
[09:10] I want to know one thing
[09:11] that's overhyped about agents right now
[09:13] and also one thing that's underhyped,
[09:15] just in terms of implementations
[09:17] or actual uses in production
[09:19] or potentials here as well.
[09:21] So Erik, let's start with you first.
[09:24] - I feel like underhyped
[09:25] is things that save people time,
[09:27] even if it's a very small amount of time.
[09:30] I think a lot of times
[09:31] if you just look at that on the surface,
[09:33] it's like, oh, this is something
that takes me a minute,
[09:35] and even if you can fully
automate it it's only a minute.
[09:37] Like, what help is that?
[09:39] But really that changes the dynamics
[09:42] of now you can do that
thing a hundred times more
[09:45] than you previously would.
[09:46] So I think I'm like most
excited about things
[09:49] that if they were easier
could be really scaled up.
[09:52] - Yeah, I don't know if this
is necessarily related to hype,
[09:54] but I think it's really
difficult to calibrate right now,
[09:57] like where agents are really needed.
[09:59] I think there's this
intersection that's a sweet spot
[10:02] for using agent and that's a set of tasks
[10:04] that's valuable and complex,
[10:06] but also maybe the cost of error
[10:08] or cost of monitoring
error is relatively low.
[10:11] That set of tasks is not
super clear and obvious,
[10:16] unless, you know, we actually look into,
[10:19] the existing processes.
[10:20] I think coding and search
[10:22] are two pretty canonical examples
[10:24] where agents are very useful.
[10:27] Like, take search as example, right?
[10:28] Like, you know, it's a
really valuable task.
[10:31] It's very hard to do
deep, iterative search,
[10:34] but you can always trade off
some precision for recall
[10:36] and then just get a
little bit more documents
[10:38] or a little bit more
information than is needed
[10:40] and filter it down.
[10:41] So we've seen a lot of success there
[10:43] with agentic search.
[10:43] - What does a coding
agent look like right now?
[10:46] - Coding agents I think are super exciting
[10:48] because they're verifiable,
at least partially.
[10:52] You know, code has this great property
[10:53] that you can write tests for
it and then you edit the code
[10:56] and either the tests
pass or they don't pass.
[10:58] Now, that assumes that
you have good unit tests,
[11:00] which I think, you know,
every engineer in the world
[11:02] can say like, we don't.
- Yeah.
[11:04] - But at least it's better
than a lot of things.
[11:06] You know, there's no
equivalent way to do that
[11:08] for many other fields.
[11:10] So this at least gives
a coding agent some way
[11:14] that it can get more signal
[11:15] every time it goes through a loop.
[11:17] So, you know, if every time
it's running the tests again
[11:21] it's seeing what the
error or the output is,
[11:23] that makes me think that, you know,
[11:25] the model can kind of
converge on the right answer
[11:27] by getting this feedback.
[11:29] And if you don't have some mechanism
[11:32] to get feedback as you're iterating,
[11:35] you're not injecting any more signal,
[11:37] you're just gonna have noise.
[11:39] And so there's no reason
without something like this
[11:42] that an agent will converge
to the right answer.
[11:44] - I see.
[11:45] So what's the biggest blockers then
[11:47] in terms of improving agentic performance
[11:50] on coding at the moment?
[11:51] - Yeah, so I think for coding,
[11:54] you know, we've seen over the
last year, like on SWE-bench,
[11:56] results have gone really
from very, very low
[11:59] to, I think you know, over 50% now,
[12:03] which is really incredible.
[12:04] So the models are getting
really good at writing code
[12:08] to solve these issues.
[12:10] I feel like I have a slightly
controversial take here
[12:12] that I think the next limiting factor
[12:14] is gonna come back to that verification.
[12:16] That, it's great for these cases
[12:19] where we do have perfect unit tests
[12:21] and that's starting to work,
but for the real world cases
[12:24] we usually don't have
perfect unit tests for them.
[12:27] And so that's what I'm thinking now,
[12:28] finding ways that we can verify
[12:31] and we can add tests for the things
[12:33] that you really care about
[12:34] so that the model itself can test this
[12:37] and know whether it's right or wrong
[12:39] before it goes back to the human.
[12:40] - I see.
[12:41] Making sure that we can embed
some sort of feedback loop
[12:44] into the processes itself.
[12:45] - Exactly.
- The right or wrong.
[12:47] Okay.
[12:49] What's the future of
agents look like in 2025?
[12:52] Barry, we're gonna start with you.
[12:54] - Yeah, I think that's a
really difficult question.
[12:56] This is probably not a practical thing,
[12:59] but one thing I've been
really interested in just
[13:01] how a multi-agent
environment would look like.
[13:04] I think I've already shown Erik this,
[13:06] I built an environment
where a bunch of Claudes
[13:08] can spin up other Claudes
[13:10] and play Werewolf together.
[13:12] And it is a completely-
[13:13] - What is Werewolf?
[13:14] - Werewolf is a social deduction game
[13:16] where all of the players
[13:18] are trying to figure out
what each other's role is.
[13:20] It's very similar to Mafia,
it's entirely text-based,
[13:23] which is great for Claude to play in.
[13:25] - I see, so we have Claudes,
[13:27] multiple different Claudes
playing different roles
[13:29] within this game, all
communicating with each other.
[13:31] - Yeah, exactly.
[13:32] And then you, you see a lot
of interesting interaction
[13:35] in there that you just haven't seen before
[13:37] and that's something
I'm really excited about
[13:39] as you know, very similar to how we went
[13:41] from single LLM to multi-LLM,
[13:43] I think by the end of the
year we could potentially see
[13:45] us going from agent to multi-agent.
[13:48] And there are some I think
interesting research questions
[13:50] to figure out in that domain.
[13:52] - In terms of how the agents
interact with each other,
[13:54] what is this kind of
emergent behavior look like
[13:57] in that front as you
coordinate between agents
[13:59] doing different things.
[14:00] - Exactly.
[14:01] And just whether this is
actually gonna be useful
[14:04] or better than a single agent
[14:05] with access to a lot more resources.
[14:07] - Do we see any multi-agent
approaches right now
[14:10] that are actually working in production?
[14:13] - I feel like in production
[14:14] we haven't even seen a lot
of successful single agents.
[14:17] - Okay, interesting.
[14:18] - But, you know,
[14:19] this is kind of a potential
extension of successful agents
[14:23] with the, I guess improved capabilities
[14:27] of the next couple of
generations of models.
[14:30] Yeah, so this is not advice
[14:32] that everyone should go explore
voting agent environment.
[14:34] It's just, I think, you know,
[14:36] to understand the model's behavior,
[14:37] this provides us with a better way
[14:39] to understand model behaviors.
[14:41] - I see.
[14:42] Okay, Erik, what's the
future of agents in 2025?
[14:44] - Yeah, I feel like in
2025 we're gonna see
[14:47] a lot of business adoption of agents,
[14:50] starting to automate a
lot of repetitive tasks
[14:52] and really scale up a lot of things
[14:54] that people wanted to do more of before
[14:57] but were too expensive.
[14:58] You could now have 10X or 100X
[14:59] how much you do with these things.
[15:02] I'm imagining things, you know,
[15:04] every single pull request
in triggers a coding agent
[15:07] to come and update all
of your documentation.
[15:09] Things like that would be
cost-prohibitive to do before.
[15:12] But once you think of agents
as sort of almost free,
[15:15] you can start doing these,
[15:16] you know, adding these bells
and whistles everywhere.
[15:19] I think maybe something
that's not gonna happen yet,
[15:20] going back to like what's overhyped?
[15:22] I feel like agents for consumers
[15:24] are fairly overhyped right now.
[15:25] - Okay, here we go. Hot take.
[15:27] - 'Cause I think that,
[15:28] we talked about verifiability.
[15:31] I think that for a lot of consumer tasks
[15:35] it's almost as much work
[15:36] to sort of fully specify your preferences
[15:38] and what the task is as
to just do it yourself,
[15:41] and it's very expensive to verify.
[15:43] So trying to have an agent
[15:45] fully book a vacation for you,
[15:47] describing exactly what you
want your vacation to be
[15:49] and your preferences
is almost just as hard
[15:52] as just going and booking it yourself.
[15:54] And it's very high risk.
[15:55] You don't want the agent to go
[15:56] actually go book a plane flight
[15:58] without you first accepting it.
[16:00] - Is there a matter of maybe context
[16:03] that we're missing here
too from the models
[16:05] being able to infer this
information about somebody
[16:07] without having to explicitly go ask
[16:09] and learn the preference over time?
[16:11] - Yeah, so I think that
these things will get there,
[16:13] but first you need to
build up this context
[16:16] so that the model already
knows your preferences
[16:17] in these things and I
think that takes time.
[16:19] - I see.
[16:20] - And we'll need some stepping stones
[16:21] to get to bigger tasks like
planning a whole vacation.
[16:23] - I see.
[16:24] Okay. Very interesting.
[16:27] Last question, any advice
that you'd give to a developer
[16:30] that's exploring this right now
[16:32] in terms of starting to build
this or just thinking about it
[16:34] from a general future-proofing perspective
[16:37] that you can give?
[16:38] - I feel like my best advice is,
[16:40] make sure that you have a
way to measure your results.
[16:44] 'Cause I've seen a lot of people will go
[16:45] and sort of build in a vacuum
[16:47] without any way to get feedback
[16:49] about whether their
building is working or not.
[16:52] And you can end up building a
lot sort of without realizing
[16:55] that either it's not working
or maybe something much simpler
[16:58] would've actually done just as good a job.
[17:01] - Yeah, I think very similarly, you know,
[17:03] starting as simple as possible
[17:05] and having that measurable
result as you're, you know,
[17:07] building more complexity into it.
[17:10] One thing I've been really
impressed by is just,
[17:12] I work with some really
resourceful startups
[17:14] and they can do everything
within one LLM call
[17:17] and the orchestration around the code,
[17:20] which will persist even
as the model gets better,
[17:23] is kind of their niche.
[17:25] And I always get very happy
when I see one of those
[17:28] because I think they can reap the benefit
[17:30] of future capability improvements.
[17:33] And yeah, I think realistically, you know,
[17:36] we don't know what use case
will be great for agents
[17:39] and the landscape's gonna shift,
[17:41] but it's probably a good time
[17:43] to start building up some of that muscle
[17:45] to think in the agent land,
[17:48] just to understand that
capability a little bit better.
[17:50] - Yeah, I think I wanna
double-click on something you said
[17:52] of, being excited for
the models to get better.
[17:55] I think that if you look at
your startup or your product
[17:58] and think, oh man, if
the models get smarter,
[18:00] all of our moat's gonna disappear.
[18:02] That means you're
building the wrong thing.
[18:04] Instead you should be building something
[18:05] so that as the models get smarter,
[18:06] your product gets better and better.
[18:08] - Right. That's great advice.
[18:11] Erik, Barry, thank you guys.
[18:12] This is Building effective
agents. Thank you.
[18:15] - Thanks.
